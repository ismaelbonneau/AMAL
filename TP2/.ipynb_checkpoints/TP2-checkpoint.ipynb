{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 2: Graphe de calcul, Optimiseur et Module\n",
    "\n",
    "_Ismaël Bonneau & Issam Benamara_\n",
    "\n",
    "Le vrai code avec en particulier les import, chargement des données, etc se trouve dans le fichier tp2.py\n",
    "\n",
    "### Descente de gradient sans optimiseur:\n",
    "\n",
    "On doit mettre à jour nous même les paramètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressor(torch.nn.Module):\n",
    "    def __init__(self, inputSize):\n",
    "        super(LinearRegressor, self).__init__()\n",
    "        #self.linear = torch.nn.Linear(inputSize, 1)\n",
    "        self.W = torch.nn.Parameter(torch.randn(inputSize, 1), requires_grad=True)\n",
    "        self.b = torch.nn.Parameter(torch.randn(1), requires_grad=True)\n",
    "    def forward(self, x): \n",
    "        #y_pred = self.linear(x) \n",
    "        y_pred = x @ self.W + self.b\n",
    "        return y_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegressor(6)\n",
    "lossfn = torch.nn.MSELoss()\n",
    "\n",
    "for e in range(epochs):\n",
    "    ohplai = [] # pour calculer ensuite la moyenne des loss sur tous les batches\n",
    "    for x_batch, y_batch in trainloader: # batch\n",
    "        model.train()\n",
    "        # forward\n",
    "        mult = model(x_batch)\n",
    "        loss = lossfn(mult, y_batch)\n",
    "        ohplai.append(loss.item())\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            model.W -= learningRate * model.W.grad\n",
    "            model.b -= learningRate * model.b.grad\n",
    "        model.W.grad.zero_() #remise a zero des gradients\n",
    "        model.b.grad.zero_()\n",
    "    with torch.no_grad():\n",
    "        # compute validation error\n",
    "        model.eval()\n",
    "        arouf = model(torch.from_numpy(X_test))\n",
    "        loss_arouf = lossfn(arouf, torch.from_numpy(Y_test))\n",
    "\n",
    "    if (e % 10) == 0:\n",
    "        print(\"epoch %d \" % e , \"train MSE: \", np.array(ohplai).mean(), \"val MSE: \", loss_arouf.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descente de gradient avec optimizer (SGD):\n",
    "\n",
    "Maintenant c'est l'optimizer qui se charge de mettre à jour les paramètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegressor(6)\n",
    "lossfn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learningRate)\n",
    "\n",
    "for e in range(epochs):\n",
    "    for x_batch, y_batch in trainloader:\n",
    "\n",
    "        model.train()\n",
    "        # forward\n",
    "        mult = model(x_batch)\n",
    "        loss = lossfn(mult, y_batch)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"boston.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version avec un NN simple à deux couches:\n",
    "\n",
    "La procédure pour la descente de gradient est la même que précédemment.\n",
    "\n",
    "Le conteneur pytorch _Sequential_ permet d'encapsuler plusieurs modules pytorch, dans un certain ordre. L'avantage: les paramètres de ces modules seront automatiquement enregistrés dans le registre des paramètres pytorch, ce qui n'est pas le cas si on utilise une liste par exemple. Autre avantage: il dispose d'une méthode forward, un seul appel permet d'exécuter en chaine un forward sur tous les modules qu'il contient. Il suffira d'un seul\n",
    "\n",
    "```python\n",
    "return mon_sequential(x)\n",
    "```\n",
    "\n",
    "#### Sans conteneur Sequential:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceval1(torch.nn.Module):\n",
    "    def __init__(self, inputSize):\n",
    "        super(Perceval, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(inputSize, 16)\n",
    "        self.linear2 = torch.nn.Linear(16, 1)\n",
    "        self.activation = torch.nn.Tanh()\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.activation(self.linear1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avec conteneur Sequential:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceval(torch.nn.Module):\n",
    "    def __init__(self, inputSize):\n",
    "        super(Perceval, self).__init__()\n",
    "        self.mlp = torch.nn.Sequential(torch.nn.Linear(inputSize, 16), torch.nn.Tanh(), torch.nn.Linear(16, 1))\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x) # un seul forward pour faire linear->tanh->linear!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"boston_perceval.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
